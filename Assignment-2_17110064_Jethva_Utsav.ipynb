{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"Project Gutenberg's The Complete Works of Jane Austen, by Jane Austen.txt\",\"r\")\n",
    "\n",
    "text=f.read().replace(\"\\n\",\" \")\n",
    "text=text.lower()\n",
    "\n",
    "#parse dataset\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "token_sent = [ word_tokenize(sent) for sent in sent_tokenize_list]\n",
    "token_list = []\n",
    "for line in token_sent:\n",
    "    token_list.append([ word for word in line if word.isalpha()])\n",
    "\n",
    "token_list = [ [\"<s>\"] + line + [\"</s>\"] for line in token_list ]\n",
    "token_sent_train ,token_sent_test = train_test_split(token_list,test_size=0.2,random_state=32)\n",
    "\n",
    "flat_token_list_train = [item for sublist in token_sent_train for item in sublist]\n",
    "flat_token_list_test = [item for sublist in token_sent_test for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of unigrams in corpus\n",
    "unigram=Counter(flat_token_list_train)\n",
    "\n",
    "#thus, sze of the vocabulary for given corpus is,\n",
    "vocabulary_length=len(unigram)\n",
    "\n",
    "#bigrams\n",
    "bigrams=Counter()\n",
    "for i in range(len(flat_token_list_train)-1):\n",
    "    bigrams_tokens=(flat_token_list_train[i],flat_token_list_train[i+1])\n",
    "    bigrams[bigrams_tokens]=bigrams.get(bigrams_tokens,0)+1   \n",
    "        \n",
    "#trigrams\n",
    "trigrams=Counter()\n",
    "for i in range(len(flat_token_list_train)-2):\n",
    "    trigrams_tokens=(flat_token_list_train[i],flat_token_list_train[i+1],flat_token_list_train[i+2])\n",
    "    trigrams[trigrams_tokens]=trigrams.get(trigrams_tokens,0)+1\n",
    "\n",
    "#quadgrams\n",
    "quadgrams=Counter()\n",
    "for i in range(len(flat_token_list_train)-3):\n",
    "    quadgrams_tokens=(flat_token_list_train[i],flat_token_list_train[i+1],flat_token_list_train[i+2],flat_token_list_train[i+3])\n",
    "    quadgrams[quadgrams_tokens]=quadgrams.get(quadgrams_tokens,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum likelihood estimation\n",
    "\n",
    "#unigram\n",
    "unigram_mle={}\n",
    "unigram_tokens=0\n",
    "for i in unigram.keys():\n",
    "    unigram_tokens+=unigram[i]\n",
    "\n",
    "for i in unigram.keys():\n",
    "    unigram_mle[i]=unigram[i]/unigram_tokens\n",
    "\n",
    "#bigrams\n",
    "bigrams_mle={}\n",
    "for (i,j) in bigrams.keys():\n",
    "    bigrams_mle[(i,j)]=bigrams[(i,j)]/unigrams[i]\n",
    "\n",
    "#trigrams\n",
    "trigrams_mle={}\n",
    "for (i,j,k) in trigrams.keys():\n",
    "    trigrams_mle[(i,j,k)]=trigrams[(i,j,k)]/bigrams[(i,j)]\n",
    "\n",
    "#quadgrams\n",
    "quadgrams_mle={}\n",
    "for (i,j,k,l) in quadgrams.keys():\n",
    "    quadgrams_mle[(i,j,k,l)]=quadgrams[(i,j,k,l)]/trigrams[(i,j,k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Actual     Possible            \n",
      "          ------     --------            \n",
      "unigram   13553      13553               \n",
      "bigram    182939     183683809           \n",
      "trigram   440443     2489466663377       \n",
      "quadgram  589948     33739741688748481   \n"
     ]
    }
   ],
   "source": [
    "print(\"          {0: <10} {1: <20}\".format(\"Actual\", \"Possible\"))\n",
    "print(\"          {0: <10} {1: <20}\".format(\"------\", \"--------\"))\n",
    "print(\"unigram   {0: <10} {1: <20}\".format(vocabulary_length, vocabulary_length))\n",
    "print(\"bigram    {0: <10} {1: <20}\".format(len(bigrams), vocabulary_length**2))\n",
    "print(\"trigram   {0: <10} {1: <20}\".format(len(trigrams), vocabulary_length**3))\n",
    "print(\"quadgram  {0: <10} {1: <20}\".format(len(quadgrams), vocabulary_length**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(n,sentence):\n",
    "    prob=0\n",
    "    for i in range(len(sentence)-n):\n",
    "        try:\n",
    "            if(n==1):\n",
    "                prob+=np.log2(unig_mle[sentence[i]])\n",
    "            if(n==2):\n",
    "                prob+=np.log2(big_mle[(sentence[i],sentence[i+1])])\n",
    "            if(n==3):\n",
    "                prob+=np.log2(trig_mle[(sentence[i],sentence[i+1],sentence[i+2])])\n",
    "            if(n==4):\n",
    "                prob+=np.log2(quad_mle[(sentence[i],sentence[i+1],sentence[i+2],sentence[i+3])])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    l =  prob/len(sentence)\n",
    "    return np.power(2, -l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_perplexity(n, test_dataset):\n",
    "    test_per=0\n",
    "    for sentence in test_dataset:\n",
    "        test_per+=perplexity(n,sentence)\n",
    "    return test_per/len(test_dataset)\n",
    "\n",
    "def test_perplexity(n, test_dataset):\n",
    "    test_per=0\n",
    "    for sentence in test_dataset:\n",
    "        test_per+=perplexity(n,sentence)\n",
    "    return test_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Perplexity           Mean Perplexity     \n",
      "           ----------           ---------------\n",
      " unigram   2632491.4177322015   375.42661405193974\n",
      " bigrams   169539.39986716444   24.178465468791277\n",
      " trigrams  18838.533056659293   2.6866133851482163\n",
      " quadgrams 8407.97183601191     1.1990832624090002\n"
     ]
    }
   ],
   "source": [
    "print(\"           {0: <20} {1: <20}\".format(\"Perplexity\", \"Mean Perplexity\"))\n",
    "print(\"           {0: <20} {1: <10}\".format(\"----------\", \"---------------\"))\n",
    "print(\" unigram   {0: <20} {1: <10}\".format(test_perplexity(1, token_sent_test),test_mean_perplexity(1, token_sent_test)))\n",
    "print(\" bigrams   {0: <20} {1: <10}\".format(test_perplexity(2, token_sent_test),test_mean_perplexity(2, token_sent_test)))\n",
    "print(\" trigrams  {0: <20} {1: <10}\".format(test_perplexity(3, token_sent_test),test_mean_perplexity(3, token_sent_test)))\n",
    "print(\" quadgrams {0: <20} {1: <10}\".format(test_perplexity(4, token_sent_test),test_mean_perplexity(4, token_sent_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findnextword(probability,next_word): #function to predict next word using multinomial distribution\n",
    "    sum=0\n",
    "    for item in probability:\n",
    "        sum+=item\n",
    "    for item in probability:\n",
    "        item=item/sum\n",
    "    lst=np.random.multinomial(1,probability,size=1).tolist()\n",
    "    return next_word[lst[0].index(1)]\n",
    "\n",
    "def bigramsprediction(current_word): #predicted word using bigrams\n",
    "    probability=[]\n",
    "    next_word=[]\n",
    "    for (b1,b2) in bigrams_mle.keys():\n",
    "        if (b1==current_word):\n",
    "            probability.append(bigrams_mle[(b1,b2)])\n",
    "            next_word.append(b2)\n",
    "    return probability,next_word\n",
    "\n",
    "def trigramsprediction(current_word,current_word1): #predicted word using trigrams\n",
    "    probability=[]\n",
    "    next_word=[]\n",
    "    for (t1,t2,t3) in trigrams_mle.keys():\n",
    "        if ((t1,t2)==(current_word,current_word1)):\n",
    "            probability.append(trigrams_mle[(t1,t2,t3)])\n",
    "            next_word.append(t3)\n",
    "    return probability,next_word\n",
    "\n",
    "def quadgramsprediction(current_word,current_word1,current_word2): ##predicted word using quadgrams\n",
    "    probability=[]\n",
    "    next_word=[]\n",
    "    for (q1,q2,q3,q4) in quadgrams_mle.keys():\n",
    "        if ((q1,q2,q3)==(current_word,current_word1,current_word2)):\n",
    "            probability.append(quadgrams_mle[(q1,q2,q3,q4)])\n",
    "            next_word.append(q4)\n",
    "    return probability,next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(n): #n=2 for bigram, 3 for trigram..\n",
    "    sentence_length=random.randint(10,20) #length of sentence\n",
    "    sentence=[\"<s>\"]\n",
    "    length=0\n",
    "    current_word,current_word1,current_word2=\"<s>\",\"\",\"\"  #current words 0,1,2 for predicting next word\n",
    "    \n",
    "    if(n==2):\n",
    "        while(length<sentence_length):\n",
    "            probability,next_word = bigramsprediction(current_word)\n",
    "            new_word=findnextword(probability,next_word)\n",
    "            sentence.append(new_word)\n",
    "            current_word=new_word\n",
    "            length+=1\n",
    "            if(current_word==\"</s>\"):\n",
    "                break\n",
    "    \n",
    "    if(n==3):\n",
    "        probability,next_word=bigramsprediction(current_word)  #predict first word using bigram\n",
    "        new_word=findnextword(probability,next_word)\n",
    "        sentence.append(new_word)\n",
    "        current_word1=new_word\n",
    "        length+=1\n",
    "        \n",
    "        while(length<sentence_length): #predict all other words using trigram\n",
    "            probability,next_word=trigramsprediction(current_word,current_word1)\n",
    "            new_word=findnextword(probability,next_word)\n",
    "            sentence.append(new_word)\n",
    "            current_word=current_word1\n",
    "            current_word1=new_word\n",
    "            length+=1\n",
    "            if(current_word1==\"</s>\"):\n",
    "                break\n",
    "    if(n==4):\n",
    "        probability,next_word=bigprediction(current_word) #predict first word using bigram\n",
    "        new_word=findnextword(probability,next_word)\n",
    "        sentence.append(new_word)\n",
    "        current_word1=new_word\n",
    "        \n",
    "        probability,next_word=trigprediction(current_word,current_word1) #predict second word using trigram\n",
    "        new_word=findnextword(probability,next_word)\n",
    "        sentence.append(new_word)\n",
    "        current_word2=new_word\n",
    "        length+=2\n",
    "        \n",
    "        while(length<sentence_length): #predict all other words using quadgram\n",
    "            probability,next_word=quadgramsprediction(current_word,current_word1,current_word2)\n",
    "            new_word=findnextword(probability,next_word)\n",
    "            sentence.append(new_word)\n",
    "            current_word=current_word1\n",
    "            current_word1=current_word2\n",
    "            current_word2=new_word\n",
    "            length+=1\n",
    "            if(current_word2==\"</s>\"):\n",
    "                break\n",
    "    s=\" \".join(sentence)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram generated sentences:\n",
      "<s> elton </s>\n",
      "<s> i had lost no grammatical errors made up with a present </s>\n",
      "<s> she that </s>\n",
      "<s> pray what would have judged like to spend some difficulty on this is what am as isabella thorpe\n",
      "<s> you i shall i should be pronounced safe and stationed quite stout girl it\n",
      "\n",
      " Trigram generated sentences:\n",
      "<s> it would not regard it as if she was so hot and the door brought something more in\n",
      "<s> you are </s>\n",
      "<s> a great blessing </s>\n",
      "<s> your sister loves to laugh it off as he now is i am almost ashamed\n",
      "<s> mary exclaimed bless me </s>\n",
      "\n",
      " Quadgram generated sentences:\n",
      "<s> but julia rushworth will be here presently miss woodhouse </s>\n",
      "<s> lizzy said he what are you in any possible way that would\n",
      "<s> you mistake me you quite mistake me she replied exerting\n",
      "<s> her address to me was as much awake to the terror of a bad day amusement for you\n",
      "<s> do they </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram generated sentences:\")        \n",
    "for i in range(5):\n",
    "    Generator(2) \n",
    "print(\"\\n Trigram generated sentences:\")  \n",
    "for i in range(5):\n",
    "    Generator(3) \n",
    "print(\"\\n Quadgram generated sentences:\")  \n",
    "for i in range(5):\n",
    "    Generator(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences generated are no so readable and does not have any meaning but there seems a grammatical pattern in them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think neural network will perform better because in n-gram models if it sees a new word in test dataset it's probability will be zero but in neural model it might detect some pattern and assign some probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
